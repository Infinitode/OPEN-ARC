{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **OPEN-ARC**\n---\n\n### Project 6: News Headline Generation Model:\n**Challenge:** Create an AI model, capable of generating convincing news headlines.\n\n\n### Terms and Use:\nLearn more about the project's [LICENSE](https://github.com/Infinitode/OPEN-ARC/blob/main/LICENSE) and read our [CODE_OF_CONDUCT](https://github.com/Infinitode/OPEN-ARC/blob/main/CODE_OF_CONDUCT) before contributing to the project. You can contribute to this project from here: [https://github.com/Infinitode/OPEN-ARC/](https://github.com/Infinitode/OPEN-ARC/).\n\n---","metadata":{}},{"cell_type":"markdown","source":"Please fill out this performance sheet to help others quickly see your model's performance **(optional)**:\n\n### Performance Sheet:\n| Contributor | Architecture Type | Platform | Base Model | Dataset | BLEU-Score | Link |\n|-------------|-------------------|----------|------------|---------|----------|------|\n| Infinitode  | DistilBART  | Kaggle   | ✗  | NEWS SUMMARY | 52.8%    | [Notebook](https://github.com/Infinitode/OPEN-ARC/Project-6-NHG/project-6-nhg.ipynb) |\n| Username  | Unknown  | Kaggle   | ✗/✔  | NEWS SUMMARY | Score    | [Notebook](https://github.com) |\n\n---","metadata":{}},{"cell_type":"markdown","source":"### Model: Pre-trained DistilBART:\nThis model is a pre-trained distilled BART model. We will fine-tune it to our specific dataset, and test its performance using the BLEU score, from `Duplipy`.","metadata":{}},{"cell_type":"markdown","source":"### Import the necessary libraries\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm  # Progress bar","metadata":{"execution":{"iopub.status.busy":"2024-09-11T09:58:20.868635Z","iopub.execute_input":"2024-09-11T09:58:20.869085Z","iopub.status.idle":"2024-09-11T09:58:26.054686Z","shell.execute_reply.started":"2024-09-11T09:58:20.869041Z","shell.execute_reply":"2024-09-11T09:58:26.053873Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Load the data and pre-trained model with its tokenizer\n---","metadata":{}},{"cell_type":"code","source":"# Load your data with different encodings\nfile_path = '/kaggle/input/news-summary/news_summary.csv'\nencodings = ['utf-8', 'ISO-8859-1', 'latin1']  # List of possible encodings\n\ndata = None\nfor enc in encodings:\n    try:\n        data = pd.read_csv(file_path, encoding=enc)\n        break\n    except UnicodeDecodeError:\n        continue\n\nif data is None:\n    raise ValueError(\"Failed to decode the file with provided encodings.\")\n\n# Load the pretrained DistilBART model and tokenizer\nmodel_name = 'sshleifer/distilbart-xsum-12-1'  # Smaller, faster version of BART for summarization tasks\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T09:58:30.107682Z","iopub.execute_input":"2024-09-11T09:58:30.108217Z","iopub.status.idle":"2024-09-11T09:59:04.910505Z","shell.execute_reply.started":"2024-09-11T09:58:30.108168Z","shell.execute_reply":"2024-09-11T09:59:04.909359Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9d2660dc7442bfa565fe6b01458c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"073b007690f842c0980a58650d7e82bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f787cca41f44ada3ed423e923e85f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739b4dbc9a0e483191a3406edb8bb5f5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70d22b252554465b6dc3e4ff9c182c6"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Preprocess the input data\n---","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset, drop missing values, etc.\ndata = data[['headlines', 'text']].dropna()\n\ntexts = data['text']\nsummaries = data['headlines']\n\n# Ensure consistency in sample size\nmin_len = min(len(texts), len(summaries))\ntexts = texts[:min_len]\nsummaries = summaries[:min_len]\n\n# Tokenize the dataset\ninputs = tokenizer(texts.tolist(), max_length=1024, return_tensors='pt', padding=True, truncation=True)\nlabels = tokenizer(summaries.tolist(), max_length=128, return_tensors='pt', padding=True, truncation=True)\n\n# Split the data into training and validation sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(\n    inputs['input_ids'], labels['input_ids'], test_size=0.2, random_state=42\n)\n\n# Move data to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\ntrain_inputs = train_inputs.to(device)\ntrain_labels = train_labels.to(device)\nval_inputs = val_inputs.to(device)\nval_labels = val_labels.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T09:59:04.914637Z","iopub.execute_input":"2024-09-11T09:59:04.915521Z","iopub.status.idle":"2024-09-11T09:59:06.624287Z","shell.execute_reply.started":"2024-09-11T09:59:04.915478Z","shell.execute_reply":"2024-09-11T09:59:06.623487Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tune and train the model\n---","metadata":{}},{"cell_type":"code","source":"# Define the training function\ndef train_model(model, train_inputs, train_labels, val_inputs, val_labels, epochs=3, batch_size=2):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    model.train()\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        # Progress bar for training loop\n        train_loader = tqdm(range(0, len(train_inputs), batch_size), desc=f\"Epoch {epoch+1} Training\", leave=False)\n\n        # Training loop\n        for i in train_loader:\n            input_batch = train_inputs[i:i+batch_size]\n            label_batch = train_labels[i:i+batch_size]\n\n            # Forward pass\n            outputs = model(input_ids=input_batch, labels=label_batch)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loader.set_postfix(loss=loss.item())\n\n        avg_train_loss = total_loss / len(train_inputs)\n\n        # Validation with progress bar\n        model.eval()\n        with torch.no_grad():\n            val_loss = 0\n            val_loader = tqdm(range(0, len(val_inputs), batch_size), desc=f\"Epoch {epoch+1} Validation\", leave=False)\n\n            for i in val_loader:\n                val_input_batch = val_inputs[i:i+batch_size]\n                val_label_batch = val_labels[i:i+batch_size]\n\n                val_outputs = model(input_ids=val_input_batch, labels=val_label_batch)\n                val_loss += val_outputs.loss.item()\n\n                val_loader.set_postfix(val_loss=val_outputs.loss.item())\n\n            avg_val_loss = val_loss / len(val_inputs)\n\n        print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n\n# Train the model\ntrain_model(model, train_inputs, train_labels, val_inputs, val_labels, epochs=3, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T09:59:17.433659Z","iopub.execute_input":"2024-09-11T09:59:17.434443Z","iopub.status.idle":"2024-09-11T10:04:09.300045Z","shell.execute_reply.started":"2024-09-11T09:59:17.434404Z","shell.execute_reply":"2024-09-11T10:04:09.299117Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Training Loss: 0.2155, Validation Loss: 0.1500\n","output_type":"stream"},{"name":"stderr","text":"                                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Training Loss: 0.1038, Validation Loss: 0.1470\n","output_type":"stream"},{"name":"stderr","text":"                                                                                     ","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Training Loss: 0.0589, Validation Loss: 0.1664\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save the trained model\n---","metadata":{}},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel.save_pretrained('distilbart_summarization_model')\ntokenizer.save_pretrained('distilbart_summarization_model')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:05:42.529546Z","iopub.execute_input":"2024-09-11T10:05:42.530628Z","iopub.status.idle":"2024-09-11T10:05:44.234246Z","shell.execute_reply.started":"2024-09-11T10:05:42.530586Z","shell.execute_reply":"2024-09-11T10:05:44.233375Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'length_penalty': 0.5, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('distilbart_summarization_model/tokenizer_config.json',\n 'distilbart_summarization_model/special_tokens_map.json',\n 'distilbart_summarization_model/vocab.json',\n 'distilbart_summarization_model/merges.txt',\n 'distilbart_summarization_model/added_tokens.json',\n 'distilbart_summarization_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r model.zip '/kaggle/working/distilbart_summarization_model'","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:09:18.874999Z","iopub.execute_input":"2024-09-11T10:09:18.875524Z","iopub.status.idle":"2024-09-11T10:10:06.585270Z","shell.execute_reply.started":"2024-09-11T10:09:18.875485Z","shell.execute_reply":"2024-09-11T10:10:06.584264Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/distilbart_summarization_model/ (stored 0%)\n  adding: kaggle/working/distilbart_summarization_model/tokenizer.json (deflated 72%)\n  adding: kaggle/working/distilbart_summarization_model/config.json (deflated 60%)\n  adding: kaggle/working/distilbart_summarization_model/tokenizer_config.json (deflated 76%)\n  adding: kaggle/working/distilbart_summarization_model/merges.txt (deflated 53%)\n  adding: kaggle/working/distilbart_summarization_model/generation_config.json (deflated 45%)\n  adding: kaggle/working/distilbart_summarization_model/vocab.json (deflated 59%)\n  adding: kaggle/working/distilbart_summarization_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/distilbart_summarization_model/special_tokens_map.json (deflated 85%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate new headlines from text\n---","metadata":{}},{"cell_type":"code","source":"# Summarization inference\ndef summarize(text):\n    inputs = tokenizer([text], max_length=1024, return_tensors='pt', padding=True, truncation=True)\n    inputs = inputs.to(device)\n\n    # Generate the summary\n    summary_ids = model.generate(inputs['input_ids'], max_length=100, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Test with a new text\nnew_text = \"The quick brown fox jumps over the lazy dog. The dog, surprised, looks at the fox. They then decide to become friends and explore the forest together.\"\nprint(summarize(new_text))","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:11:28.170672Z","iopub.execute_input":"2024-09-11T10:11:28.171416Z","iopub.status.idle":"2024-09-11T10:11:28.238346Z","shell.execute_reply.started":"2024-09-11T10:11:28.171378Z","shell.execute_reply":"2024-09-11T10:11:28.237466Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Delhi fox jumps over lazy dog in a park\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Meassure the model's performance using Duplipy\n---\nYou can now test the model using Duplipy's built-in BLEU Score function.","metadata":{}},{"cell_type":"code","source":"!pip install duplipy","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:13:23.548416Z","iopub.execute_input":"2024-09-11T10:13:23.549083Z","iopub.status.idle":"2024-09-11T10:13:37.818856Z","shell.execute_reply.started":"2024-09-11T10:13:23.549043Z","shell.execute_reply":"2024-09-11T10:13:37.817817Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting duplipy\n  Downloading duplipy-0.2.2-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from duplipy) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from duplipy) (1.26.4)\nRequirement already satisfied: langcodes in /opt/conda/lib/python3.10/site-packages (from duplipy) (3.4.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from duplipy) (1.4.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from duplipy) (4.66.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from duplipy) (9.5.0)\nCollecting valx (from duplipy)\n  Downloading valx-0.2.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes->duplipy) (1.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->duplipy) (1.16.0)\nRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from valx->duplipy) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->valx->duplipy) (1.14.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->valx->duplipy) (3.5.0)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes->duplipy) (1.1.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from marisa-trie>=0.7.7->language-data>=1.2->langcodes->duplipy) (70.0.0)\nDownloading duplipy-0.2.2-py3-none-any.whl (10.0 kB)\nDownloading valx-0.2.0-py3-none-any.whl (349 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.7/349.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: valx, duplipy\nSuccessfully installed duplipy-0.2.2 valx-0.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from duplipy.similarity import bleu_score\nimport numpy as np\n\n# Evaluate using BLEU score\ndef evaluate_bleu(reference_summaries, generated_summaries):\n    bleu_scores = []\n    for ref, gen in zip(reference_summaries, generated_summaries):\n        score = bleu_score(ref, gen)\n        bleu_scores.append(score)\n    return np.mean(bleu_scores)\n\n# Generate summaries for the validation set\ngenerated_summaries = [summarize(text) for text in texts[:len(val_inputs)]]  # Adjust as needed\naverage_bleu = evaluate_bleu(summaries[:len(val_inputs)], generated_summaries)\n\nprint(f'Average BLEU Score: {average_bleu:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:13:46.125870Z","iopub.execute_input":"2024-09-11T10:13:46.126277Z","iopub.status.idle":"2024-09-11T10:14:53.597118Z","shell.execute_reply.started":"2024-09-11T10:13:46.126239Z","shell.execute_reply":"2024-09-11T10:14:53.596105Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Average BLEU Score: 0.5284\n","output_type":"stream"}]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OPEN-ARC**\n",
    "---\n",
    "\n",
    "### Project 6: News Headline Generation Model:\n",
    "**Challenge:** Create an AI model, capable of generating convincing news headlines.\n",
    "\n",
    "\n",
    "### Terms and Use:\n",
    "Learn more about the project's [LICENSE](https://github.com/Infinitode/OPEN-ARC/blob/main/LICENSE) and read our [CODE_OF_CONDUCT](https://github.com/Infinitode/OPEN-ARC/blob/main/CODE_OF_CONDUCT) before contributing to the project. You can contribute to this project from here: [https://github.com/Infinitode/OPEN-ARC/](https://github.com/Infinitode/OPEN-ARC/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out this performance sheet to help others quickly see your model's performance **(optional)**:\n",
    "\n",
    "### Performance Sheet:\n",
    "| Contributor | Architecture Type | Platform | Base Model | Dataset | BLEU-Score | Link |\n",
    "|-------------|-------------------|----------|------------|---------|----------|------|\n",
    "| Infinitode  | DistilBART  | Kaggle   | ✗  | NEWS SUMMARY | 52.8%    | [Notebook](https://github.com/Infinitode/OPEN-ARC/blob/main/Project-6-NHG/project-6-nhg.ipynb) |\n",
    "| Username  | Unknown  | Kaggle   | ✗/✔  | NEWS SUMMARY | Score    | [Notebook](https://github.com) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Pre-trained DistilBART:\n",
    "This model is a pre-trained distilled BART model. We will fine-tune it to our specific dataset, and test its performance using the BLEU score, from `Duplipy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T09:58:20.869085Z",
     "iopub.status.busy": "2024-09-11T09:58:20.868635Z",
     "iopub.status.idle": "2024-09-11T09:58:26.054686Z",
     "shell.execute_reply": "2024-09-11T09:58:26.053873Z",
     "shell.execute_reply.started": "2024-09-11T09:58:20.869041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and pre-trained model with its tokenizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T09:58:30.108217Z",
     "iopub.status.busy": "2024-09-11T09:58:30.107682Z",
     "iopub.status.idle": "2024-09-11T09:59:04.910505Z",
     "shell.execute_reply": "2024-09-11T09:59:04.909359Z",
     "shell.execute_reply.started": "2024-09-11T09:58:30.108168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9d2660dc7442bfa565fe6b01458c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b007690f842c0980a58650d7e82bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f787cca41f44ada3ed423e923e85f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739b4dbc9a0e483191a3406edb8bb5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70d22b252554465b6dc3e4ff9c182c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your data with different encodings\n",
    "file_path = '/kaggle/input/news-summary/news_summary.csv'\n",
    "encodings = ['utf-8', 'ISO-8859-1', 'latin1']  # List of possible encodings\n",
    "\n",
    "data = None\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding=enc)\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "if data is None:\n",
    "    raise ValueError(\"Failed to decode the file with provided encodings.\")\n",
    "\n",
    "# Load the pretrained DistilBART model and tokenizer\n",
    "model_name = 'sshleifer/distilbart-xsum-12-1'  # Smaller, faster version of BART for summarization tasks\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the input data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T09:59:04.915521Z",
     "iopub.status.busy": "2024-09-11T09:59:04.914637Z",
     "iopub.status.idle": "2024-09-11T09:59:06.624287Z",
     "shell.execute_reply": "2024-09-11T09:59:06.623487Z",
     "shell.execute_reply.started": "2024-09-11T09:59:04.915478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset, drop missing values, etc.\n",
    "data = data[['headlines', 'text']].dropna()\n",
    "\n",
    "texts = data['text']\n",
    "summaries = data['headlines']\n",
    "\n",
    "# Ensure consistency in sample size\n",
    "min_len = min(len(texts), len(summaries))\n",
    "texts = texts[:min_len]\n",
    "summaries = summaries[:min_len]\n",
    "\n",
    "# Tokenize the dataset\n",
    "inputs = tokenizer(texts.tolist(), max_length=1024, return_tensors='pt', padding=True, truncation=True)\n",
    "labels = tokenizer(summaries.tolist(), max_length=128, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    inputs['input_ids'], labels['input_ids'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Move data to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "train_inputs = train_inputs.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "val_inputs = val_inputs.to(device)\n",
    "val_labels = val_labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune and train the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T09:59:17.434443Z",
     "iopub.status.busy": "2024-09-11T09:59:17.433659Z",
     "iopub.status.idle": "2024-09-11T10:04:09.300045Z",
     "shell.execute_reply": "2024-09-11T10:04:09.299117Z",
     "shell.execute_reply.started": "2024-09-11T09:59:17.434404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2155, Validation Loss: 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.1038, Validation Loss: 0.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.0589, Validation Loss: 0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Define the training function\n",
    "def train_model(model, train_inputs, train_labels, val_inputs, val_labels, epochs=3, batch_size=2):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # Progress bar for training loop\n",
    "        train_loader = tqdm(range(0, len(train_inputs), batch_size), desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "\n",
    "        # Training loop\n",
    "        for i in train_loader:\n",
    "            input_batch = train_inputs[i:i+batch_size]\n",
    "            label_batch = train_labels[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_batch, labels=label_batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loader.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_inputs)\n",
    "\n",
    "        # Validation with progress bar\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_loader = tqdm(range(0, len(val_inputs), batch_size), desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "\n",
    "            for i in val_loader:\n",
    "                val_input_batch = val_inputs[i:i+batch_size]\n",
    "                val_label_batch = val_labels[i:i+batch_size]\n",
    "\n",
    "                val_outputs = model(input_ids=val_input_batch, labels=val_label_batch)\n",
    "                val_loss += val_outputs.loss.item()\n",
    "\n",
    "                val_loader.set_postfix(val_loss=val_outputs.loss.item())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_inputs)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_inputs, train_labels, val_inputs, val_labels, epochs=3, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:05:42.530628Z",
     "iopub.status.busy": "2024-09-11T10:05:42.529546Z",
     "iopub.status.idle": "2024-09-11T10:05:44.234246Z",
     "shell.execute_reply": "2024-09-11T10:05:44.233375Z",
     "shell.execute_reply.started": "2024-09-11T10:05:42.530586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'length_penalty': 0.5, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('distilbart_summarization_model/tokenizer_config.json',\n",
       " 'distilbart_summarization_model/special_tokens_map.json',\n",
       " 'distilbart_summarization_model/vocab.json',\n",
       " 'distilbart_summarization_model/merges.txt',\n",
       " 'distilbart_summarization_model/added_tokens.json',\n",
       " 'distilbart_summarization_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('distilbart_summarization_model')\n",
    "tokenizer.save_pretrained('distilbart_summarization_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:09:18.875524Z",
     "iopub.status.busy": "2024-09-11T10:09:18.874999Z",
     "iopub.status.idle": "2024-09-11T10:10:06.585270Z",
     "shell.execute_reply": "2024-09-11T10:10:06.584264Z",
     "shell.execute_reply.started": "2024-09-11T10:09:18.875485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/distilbart_summarization_model/ (stored 0%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/tokenizer.json (deflated 72%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/config.json (deflated 60%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/tokenizer_config.json (deflated 76%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/merges.txt (deflated 53%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/generation_config.json (deflated 45%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/vocab.json (deflated 59%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/distilbart_summarization_model/special_tokens_map.json (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model.zip '/kaggle/working/distilbart_summarization_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new headlines from text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:11:28.171416Z",
     "iopub.status.busy": "2024-09-11T10:11:28.170672Z",
     "iopub.status.idle": "2024-09-11T10:11:28.238346Z",
     "shell.execute_reply": "2024-09-11T10:11:28.237466Z",
     "shell.execute_reply.started": "2024-09-11T10:11:28.171378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi fox jumps over lazy dog in a park\n"
     ]
    }
   ],
   "source": [
    "# Summarization inference\n",
    "def summarize(text):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', padding=True, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=100, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Test with a new text\n",
    "new_text = \"The quick brown fox jumps over the lazy dog. The dog, surprised, looks at the fox. They then decide to become friends and explore the forest together.\"\n",
    "print(summarize(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meassure the model's performance using Duplipy\n",
    "---\n",
    "You can now test the model using Duplipy's built-in BLEU Score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:13:23.549083Z",
     "iopub.status.busy": "2024-09-11T10:13:23.548416Z",
     "iopub.status.idle": "2024-09-11T10:13:37.818856Z",
     "shell.execute_reply": "2024-09-11T10:13:37.817817Z",
     "shell.execute_reply.started": "2024-09-11T10:13:23.549043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duplipy\n",
      "  Downloading duplipy-0.2.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from duplipy) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from duplipy) (1.26.4)\n",
      "Requirement already satisfied: langcodes in /opt/conda/lib/python3.10/site-packages (from duplipy) (3.4.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from duplipy) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from duplipy) (4.66.4)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from duplipy) (9.5.0)\n",
      "Collecting valx (from duplipy)\n",
      "  Downloading valx-0.2.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes->duplipy) (1.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->duplipy) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from valx->duplipy) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->valx->duplipy) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->valx->duplipy) (3.5.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes->duplipy) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from marisa-trie>=0.7.7->language-data>=1.2->langcodes->duplipy) (70.0.0)\n",
      "Downloading duplipy-0.2.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading valx-0.2.0-py3-none-any.whl (349 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.7/349.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: valx, duplipy\n",
      "Successfully installed duplipy-0.2.2 valx-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install duplipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:13:46.126277Z",
     "iopub.status.busy": "2024-09-11T10:13:46.125870Z",
     "iopub.status.idle": "2024-09-11T10:14:53.597118Z",
     "shell.execute_reply": "2024-09-11T10:14:53.596105Z",
     "shell.execute_reply.started": "2024-09-11T10:13:46.126239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.5284\n"
     ]
    }
   ],
   "source": [
    "from duplipy.similarity import bleu_score\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate using BLEU score\n",
    "def evaluate_bleu(reference_summaries, generated_summaries):\n",
    "    bleu_scores = []\n",
    "    for ref, gen in zip(reference_summaries, generated_summaries):\n",
    "        score = bleu_score(ref, gen)\n",
    "        bleu_scores.append(score)\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "# Generate summaries for the validation set\n",
    "generated_summaries = [summarize(text) for text in texts[:len(val_inputs)]]  # Adjust as needed\n",
    "average_bleu = evaluate_bleu(summaries[:len(val_inputs)], generated_summaries)\n",
    "\n",
    "print(f'Average BLEU Score: {average_bleu:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End:\n",
    "\n",
    "This is the end of this project notebook, make sure to experiment and contribute to help improve the model and implementation. You can browse more of the open-source free projects on our GitHub repository: https://github.com/Infinitode/OPEN-ARC. If you like this project, make sure to star the repo and contribute your implementation, or help others in the community.\n",
    "\n",
    "~ Infinitode"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1895,
     "sourceId": 791838,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
